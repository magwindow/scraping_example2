{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna\n",
      "Pavlovna Scherer\n",
      "Empress Marya\n",
      "Fedorovna\n",
      "Prince Vasili Kuragin\n",
      "Anna Pavlovna\n",
      "St. Petersburg\n",
      "the prince\n",
      "Anna Pavlovna\n",
      "Anna Pavlovna\n",
      "the prince\n",
      "the prince\n",
      "the prince\n",
      "Prince Vasili\n",
      "Anna Pavlovna\n",
      "Anna Pavlovna\n",
      "the prince\n",
      "Wintzingerode\n",
      "King of Prussia\n",
      "le Vicomte de Mortemart\n",
      "Montmorencys\n",
      "Rohans\n",
      "Abbe Morio\n",
      "the Emperor\n",
      "the prince\n",
      "Prince Vasili\n",
      "Dowager Empress Marya Fedorovna\n",
      "the baron\n",
      "Anna Pavlovna\n",
      "the Empress\n",
      "the Empress\n",
      "Anna Pavlovna's\n",
      "Her Majesty\n",
      "Baron\n",
      "Funke\n",
      "The prince\n",
      "Anna\n",
      "Pavlovna\n",
      "the Empress\n",
      "The prince\n",
      "Anatole\n",
      "the prince\n",
      "The prince\n",
      "Anna\n",
      "Pavlovna\n",
      "Anna Pavlovna\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('https://www.pythonscraping.com/pages/warandpeace.html')\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "namelist = bs.find_all('span', {'class': 'green'})\n",
    "\n",
    "for name in namelist:\n",
    "    print(name.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "namelist = bs.find_all(string='the prince')\n",
    "print(len(namelist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<tr><th>\n",
      "Item Title\n",
      "</th><th>\n",
      "Description\n",
      "</th><th>\n",
      "Cost\n",
      "</th><th>\n",
      "Image\n",
      "</th></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift1\"><td>\n",
      "Vegetable Basket\n",
      "</td><td>\n",
      "This vegetable basket is the perfect gift for your health conscious (or overweight) friends!\n",
      "<span class=\"excitingNote\">Now with super-colorful bell peppers!</span>\n",
      "</td><td>\n",
      "$15.00\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img1.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift2\"><td>\n",
      "Russian Nesting Dolls\n",
      "</td><td>\n",
      "Hand-painted by trained monkeys, these exquisite dolls are priceless! And by \"priceless,\" we mean \"extremely expensive\"! <span class=\"excitingNote\">8 entire dolls per set! Octuple the presents!</span>\n",
      "</td><td>\n",
      "$10,000.52\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img2.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift3\"><td>\n",
      "Fish Painting\n",
      "</td><td>\n",
      "If something seems fishy about this painting, it's because it's a fish! <span class=\"excitingNote\">Also hand-painted by trained monkeys!</span>\n",
      "</td><td>\n",
      "$10,005.00\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img3.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift4\"><td>\n",
      "Dead Parrot\n",
      "</td><td>\n",
      "This is an ex-parrot! <span class=\"excitingNote\">Or maybe he's only resting?</span>\n",
      "</td><td>\n",
      "$0.50\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img4.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift5\"><td>\n",
      "Mystery Box\n",
      "</td><td>\n",
      "If you love suprises, this mystery box is for you! Do not place on light-colored surfaces. May cause oil staining. <span class=\"excitingNote\">Keep your friends guessing!</span>\n",
      "</td><td>\n",
      "$1.50\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img6.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "for child in bs.find('table', {'id': 'giftList'}).children:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift1\"><td>\n",
      "Vegetable Basket\n",
      "</td><td>\n",
      "This vegetable basket is the perfect gift for your health conscious (or overweight) friends!\n",
      "<span class=\"excitingNote\">Now with super-colorful bell peppers!</span>\n",
      "</td><td>\n",
      "$15.00\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img1.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift2\"><td>\n",
      "Russian Nesting Dolls\n",
      "</td><td>\n",
      "Hand-painted by trained monkeys, these exquisite dolls are priceless! And by \"priceless,\" we mean \"extremely expensive\"! <span class=\"excitingNote\">8 entire dolls per set! Octuple the presents!</span>\n",
      "</td><td>\n",
      "$10,000.52\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img2.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift3\"><td>\n",
      "Fish Painting\n",
      "</td><td>\n",
      "If something seems fishy about this painting, it's because it's a fish! <span class=\"excitingNote\">Also hand-painted by trained monkeys!</span>\n",
      "</td><td>\n",
      "$10,005.00\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img3.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift4\"><td>\n",
      "Dead Parrot\n",
      "</td><td>\n",
      "This is an ex-parrot! <span class=\"excitingNote\">Or maybe he's only resting?</span>\n",
      "</td><td>\n",
      "$0.50\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img4.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift5\"><td>\n",
      "Mystery Box\n",
      "</td><td>\n",
      "If you love suprises, this mystery box is for you! Do not place on light-colored surfaces. May cause oil staining. <span class=\"excitingNote\">Keep your friends guessing!</span>\n",
      "</td><td>\n",
      "$1.50\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img6.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sibling in bs.find('table', {'id': 'giftList'}).tr.next_siblings:\n",
    "    print(sibling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "$15.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(bs.find('img', {'src': '../img/gifts/img1.jpg'}).parent.previous_sibling.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../img/gifts/img1.jpg\n",
      "../img/gifts/img2.jpg\n",
      "../img/gifts/img3.jpg\n",
      "../img/gifts/img4.jpg\n",
      "../img/gifts/img6.jpg\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "images = bs.find_all('img', {'src': re.compile('..\\/img\\/gifts/img.*.jpg')})\n",
    "\n",
    "for image in images:\n",
    "    print(image['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<span class=\"excitingNote\">Or maybe he's only resting?</span>]\n",
      "[<span class=\"excitingNote\">Or maybe he's only resting?</span>]\n"
     ]
    }
   ],
   "source": [
    "print(bs.find_all(lambda tag: tag.get_text() == 'Or maybe he\\'s only resting?'))\n",
    "print(bs.find_all('span', string='Or maybe he\\'s only resting?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random external link is: https://pyplanet.ru/article/telegram-bot-aiogram-quickstart.html\n",
      "Random external link is: https://zallj.com/g/qqb62fps0r99b979a60661cec322b98fba714163/?erid=5jtCeReNwxHpfQTDve31wmc\n",
      "Random external link is: https://yandex.cloud/ru/services/smartcaptcha?utm_source=captcha&utm_medium=chbx&utm_campaign=security\n",
      "Random external link is: https://yandex.ru/legal/cloud_termsofuse\n",
      "Random external link is: https://yandex.cloud\n",
      "Random external link is: https://sourcecraft.dev/\n",
      "Random external link is: https://ya.ru\n",
      "Random external link is: https://yandex.ru/support/smart-captcha/problems.html?form-unique_key=7639461546662057535&form-fb-hint=2.47\n",
      "Error http:  HTTP Error 404: Not Found\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from urllib.error import HTTPError\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_internal_links(bs, url):\n",
    "    '''Retrieves a list of all Internal links found on a page'''\n",
    "    netloc = urlparse(url).netloc\n",
    "    scheme = urlparse(url).scheme\n",
    "    internal_links = set()\n",
    "    for link in bs.find_all('a'):\n",
    "        if not link.attrs.get('href'):\n",
    "            continue\n",
    "        parsed = urlparse(link.attrs['href'])\n",
    "        if parsed.netloc == '':\n",
    "            internal_links.add(f'{scheme}://{netloc}/{link.attrs[\"href\"].strip(\"/\")}')\n",
    "        elif parsed.netloc == netloc:\n",
    "            internal_links.add(link.attrs['href'])\n",
    "    return list(internal_links)\n",
    "            \n",
    "\n",
    "def get_external_links(bs, url):\n",
    "    '''Retrieves a list of all external links found on a page'''\n",
    "    netloc = urlparse(url).netloc\n",
    "    external_links = set()\n",
    "    for link in bs.find_all('a'):\n",
    "        if not link.attrs.get('href'):\n",
    "            continue\n",
    "        parsed = urlparse(link.attrs['href'])\n",
    "        if parsed.netloc != '' and parsed.netloc != netloc:\n",
    "            external_links.add(link.attrs['href'])\n",
    "    return list(external_links)\n",
    "\n",
    "def get_random_external_link(starting_page):\n",
    "    bs = BeautifulSoup(urlopen(starting_page), 'html.parser')\n",
    "    external_links = get_external_links(bs, starting_page)\n",
    "    if not len(external_links):\n",
    "        print('No external links, looking around the site for one')\n",
    "        internal_links = get_internal_links(bs, starting_page)\n",
    "        return get_random_external_link(random.choice(internal_links))\n",
    "    else:\n",
    "        return random.choice(external_links)\n",
    "    \n",
    "def follow_external_only(starting_site):\n",
    "    try:\n",
    "        external_link = get_random_external_link(starting_site)\n",
    "        print(f'Random external link is: {external_link}')\n",
    "        follow_external_only(external_link)\n",
    "    except ValueError as e:\n",
    "        print('Error value link: ', e)\n",
    "    except HTTPError as e:\n",
    "        print('Error http: ', e)\n",
    "    except TimeoutError as e:\n",
    "        print('Connection timeout: ', e)\n",
    "    except IndexError as e:\n",
    "        print('Range index not found: ', e)\n",
    "\n",
    "\n",
    "follow_external_only('https://pythonworld.ru/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collects a list of all external URLs found on the site\n",
    "all_ext_links = []\n",
    "all_int_links = []\n",
    "\n",
    "\n",
    "def get_all_external_links(url):\n",
    "    bs = BeautifulSoup(urlopen(url), 'html.parser')\n",
    "    internal_links = get_internal_links(bs, url)\n",
    "    external_links = get_external_links(bs, url)\n",
    "    for link in external_links:\n",
    "        if link not in all_ext_links:\n",
    "            all_ext_links.append(link)\n",
    "            print(link)\n",
    "\n",
    "    for link in internal_links:\n",
    "        if link not in all_int_links:\n",
    "            all_int_links.append(link)\n",
    "            get_all_external_links(link)\n",
    "\n",
    "\n",
    "all_int_links.append('https://oreilly.com')\n",
    "get_all_external_links('https://www.oreilly.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Delivering inclusive urban access: 3 uncomfortable truths\n",
      "URL: https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "class Content:\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "        \n",
    "\n",
    "def get_page(url):\n",
    "    response = requests.get(url)\n",
    "    return BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "def scrape(url):\n",
    "    bs = get_page(url)\n",
    "    title = bs.find('h1').text\n",
    "    lines = bs.select('div.StoryBodyCompanionColumn div p')\n",
    "    body = '\\n'.join(line.text for line in lines)\n",
    "    return Content(url, title, body)\n",
    "\n",
    "\n",
    "url = 'https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/'\n",
    "\n",
    "content = scrape(url)\n",
    "print(f'Title: {content.title}')\n",
    "print(f'URL: {content.url}\\n')\n",
    "print(content.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITLE: Roboticrulemaking\n",
      "URL: https://www.brookings.edu/research/robotic-rulemaking/\n",
      "BODY: More On\n",
      "\n",
      "body: \n",
      "\n",
      "\n",
      "\n",
      "      Dogecoin jumps after Elon Musk replaces Twitter bird with Shiba Inu\n",
      "    \n",
      "\n",
      "TITLE: DogecoinjumpsafterElonMuskreplacesTwitterbirdwithShibaInu\n",
      "URL: https://www.cnn.com/2023/04/03/investing/dogecoin-elon-musk-twitter/index.html\n",
      "BODY:       Dogecoin jumps after Elon Musk replaces Twitter bird with Shiba Inu    \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "class Content:\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "    \n",
    "    def print_info(self):\n",
    "        print('TITLE:', f'{self.title}'.replace(\"\\n\", \"\").replace(\" \", \"\"))\n",
    "        print(f'URL: {self.url}')\n",
    "        print('BODY:', f'{self.body}'.replace(\"\\n\", \"\").strip(\"\\t\"))\n",
    "\n",
    "def scrapeCNN(url):\n",
    "    bs = BeautifulSoup(urlopen(url))\n",
    "    title = bs.find('h1').text\n",
    "    body = bs.find('div', {'class': 'headline__wrapper'}).text\n",
    "    print('body: ')\n",
    "    print(body)\n",
    "    return Content(url, title, body)\n",
    "\n",
    "def scrapeBrookings(url):\n",
    "    bs = BeautifulSoup(urlopen(url))\n",
    "    title = bs.find('h1').text\n",
    "    body = bs.find('div', {'class': 'label-large'}).text\n",
    "    return Content(url, title, body)\n",
    "\n",
    "url = 'https://www.brookings.edu/research/robotic-rulemaking/'\n",
    "content = scrapeBrookings(url)\n",
    "content.print_info()\n",
    "print()\n",
    "url = 'https://www.cnn.com/2023/04/03/investing/dogecoin-elon-musk-twitter/index.html'\n",
    "content = scrapeCNN(url)\n",
    "content.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://www.oreilly.com/library/view/web-scraping-with/9781491910283\n",
      "TITLE: Web Scraping with Python\n",
      "BODY:\n",
      "\n",
      "URL: https://www.reuters.com/article/us-usa-epa-pruitt-idUSKBN19W2D0\n",
      "TITLE: \n",
      "BODY:\n",
      "\n",
      "URL: https://www.brookings.edu/blog/techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/\n",
      "TITLE: Idea to Retire: Old methods of policy education\n",
      "BODY:\n",
      "\n",
      "URL: https://www.cnn.com/2023/04/03/investing/dogecoin-elon-musk-twitter/index.html\n",
      "TITLE: \n",
      "      Dogecoin jumps after Elon Musk replaces Twitter bird with Shiba Inu\n",
      "    \n",
      "BODY:\n",
      "\n",
      "\n",
      "\n",
      "New York\n",
      "CNN\n",
      "         — \n",
      "    \n",
      "\n",
      "\n",
      "            Twitter’s traditional bird icon was booted and replaced with an image of a Shiba Inu, an apparent nod to dogecoin, the joke cryptocurrency that CEO Elon Musk is being sued over. \n",
      "    \n",
      "\n",
      "            Musk addressed the change Monday afternoon, tweeting, “as promised” above an image of a year-old conversation in which another user suggested that Musk “just buy Twitter” and “change the bird logo to a doge.” \n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CNN/Adobe Stock\n",
      "\n",
      "\n",
      "\n",
      "Related article\n",
      "Elon Musk's Twitter promised a purge of blue check marks. Instead he singled out one account\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            The doge logo appeared on the site two days after Musk asked a judge to throw out a $258 billion racketeering lawsuit accusing him of running a pyramid scheme to support the dogecoin, according to Reuters.\n",
      "\n",
      "\n",
      "            Lawyers for Musk and Tesla called the lawsuit by dogecoin investors a “fanciful work of fiction” over Musk’s “innocuous and often silly tweets.”\n",
      "    \n",
      "\n",
      "            It wasn’t clear whether the logo change was permanent. Musk has been known to use Twitter to troll both his fans and his critics. \n",
      "    \n",
      "\n",
      "            The price of dogecoin, which is typically volatile, was up more than 20% over the past 24 hours, to about 9 cents. It was trading just under 8 cents Monday morning.\n",
      "    \n",
      "\n",
      "Dogecoin was created December 6, 2013, by a pair of software engineers — as a joke. The name is a nod to the “doge” meme that became popular a decade ago. Its Shiba Inu mascot mimicks that meme: a dog surrounded by a bunch of Comic Sans text in broken English.\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "\n",
    "class Content:\n",
    "    \"\"\"Общий родительский класс для всех статей/страниц.\"\"\"\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "        \n",
    "    def print_info(self):\n",
    "        \"\"\"Гибкая функция печати, управляющая выводом данных.\"\"\"\n",
    "        print(f'URL: {self.url}')\n",
    "        print(f'TITLE: {self.title}')\n",
    "        print(f'BODY:\\n{self.body}')\n",
    "        \n",
    "\n",
    "class Website:\n",
    "    \"\"\"Содержит информацию о структуре сайта.\"\"\"\n",
    "    def __init__(self, name, url, title_tag, body_tag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.title_tag = title_tag\n",
    "        self.body_tag = body_tag\n",
    "        \n",
    "\n",
    "class Crawler:\n",
    "    def get_page(self, url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "        return BS(response.text, 'html.parser')\n",
    "    \n",
    "    def safe_get(self, bs, selector):\n",
    "        \"\"\"\n",
    "        Служебная функция, используемая для получения строки \n",
    "        содержимого из объекта BeautifulSoup и селектора.\n",
    "        Если объект для данного селектора не найден,\n",
    "        то возвращает пустую строку.\n",
    "        \"\"\"\n",
    "        selected_elems = bs.select(selector)\n",
    "        if selected_elems is not None and len(selected_elems) > 0:\n",
    "            return '\\n'.join(elem.get_text() for elem in selected_elems)\n",
    "        return ''\n",
    "    \n",
    "    def get_content(self, website, path):\n",
    "        \"\"\"Извлекает содержимое страницы с заданным URL.\"\"\"\n",
    "        url = website.url + path\n",
    "        bs = self.get_page(url)\n",
    "        if bs is not None:\n",
    "            title = self.safe_get(bs, website.title_tag)\n",
    "            body = self.safe_get(bs, website.body_tag)\n",
    "            return Content(url, title, body)\n",
    "        return Content(url, '', '')\n",
    "\n",
    "\n",
    "crawler = Crawler()\n",
    "\n",
    "site_data = [\n",
    "    ['O\\'Reilly Media', 'https://www.oreilly.com', 'h1', 'div.title-description'],\n",
    "    ['Reuters', 'https://www.reuters.com', 'h1', 'div.ArticleBodyWrapper'],\n",
    "    ['Brookings', 'https://www.brookings.edu', 'h1', 'div.post-body'],\n",
    "    ['CNN', 'https://www.cnn.com', 'h1', 'div.article__content']\n",
    "]\n",
    "\n",
    "websites = []\n",
    "\n",
    "for name, url, title, body in site_data:\n",
    "    websites.append(Website(name, url, title, body))\n",
    "    \n",
    "crawler.get_content(websites[0], '/library/view/web-scraping-with/9781491910283').print_info()\n",
    "crawler.get_content(websites[1], '/article/us-usa-epa-pruitt-idUSKBN19W2D0').print_info()\n",
    "crawler.get_content(websites[2], '/blog/techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/').print_info()\n",
    "crawler.get_content(websites[3], '/2023/04/03/investing/dogecoin-elon-musk-twitter/index.html').print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
